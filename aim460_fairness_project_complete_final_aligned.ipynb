{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikemaurrasse-hash/GP2-dataset-prep/blob/main/aim460_fairness_project_complete_final_aligned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516ed6bb",
      "metadata": {
        "id": "516ed6bb"
      },
      "source": [
        "\n",
        "# AIM 460 — Fairness Project (Complete Colab)\n",
        "**Bias in Demographic Datasets: Cross-Domain Evaluation of Debiasing Methods**  \n",
        "*Author: Michael Maurrasse* • *Generated: 2025-10-18*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028a3a3d",
      "metadata": {
        "id": "028a3a3d"
      },
      "source": [
        "\n",
        "This unified notebook includes:\n",
        "- **Setup** (Colab-safe dependencies)\n",
        "- **Data loaders** (Folktables + COMPAS stub)\n",
        "- **Baselines** (logistic regression)\n",
        "- **Debiasing**: pre (DSAP reweighting), in (Equalized Odds via `fairlearn`), post (ThresholdOptimizer)\n",
        "- **Metrics** (Accuracy, AUC, Equal Opportunity, Equalized Odds)\n",
        "- **Aggregation + exports** for Overleaf\n",
        "- **CivilComments (Text)**: BERT baseline + fairness evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92445204",
      "metadata": {
        "id": "92445204"
      },
      "source": [
        "## 1) Setup (Colab-safe pins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdba9879",
      "metadata": {
        "id": "fdba9879"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Setup (Colab-safe) — Run this first, then Runtime > Restart runtime\n",
        "!pip -q uninstall -y cuml-cu12 cudf-cu12 dask-cudf-cu12 2>/dev/null || true\n",
        "!pip -q install --upgrade --no-cache-dir pandas==2.2.2\n",
        "!pip -q install --upgrade --no-cache-dir \\\n",
        "  torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install --upgrade --no-cache-dir \\\n",
        "  \"fairlearn>=0.12,<0.14\" \\\n",
        "  scikit-learn transformers==4.44.2 datasets==2.21.0 evaluate accelerate \\\n",
        "  shap folktables==0.0.12 matplotlib seaborn\n",
        "!pip -q install --upgrade --no-cache-dir fsspec==2025.3.0 gcsfs==2025.3.0\n",
        "\n",
        "import torch, pandas as pd\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "print(\"Setup complete. Now do: Runtime > Restart runtime, then continue.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Permanent fix for Folktables data not found errors ===\n",
        "!pip install -q folktables==0.0.12\n",
        "\n",
        "import os\n",
        "from folktables import ACSDataSource\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "# Force re-download all required ACS person data for Employment\n",
        "for state in [\"CA\", \"TX\", \"NY\"]:\n",
        "    try:\n",
        "        ds = ACSDataSource(survey_year=2018, horizon=\"1-Year\", survey=\"person\", root_dir=DATA_ROOT)\n",
        "        acs = ds.get_data(states=[state], download=True)\n",
        "        print(f\"✅ Downloaded {state} ({len(acs)} records)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ {state} failed:\", e)\n",
        "\n",
        "print(\"\\nAll requested ACS files downloaded to:\", DATA_ROOT)"
      ],
      "metadata": {
        "id": "l4oEFT42B5S1"
      },
      "id": "l4oEFT42B5S1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5089ec5c",
      "metadata": {
        "id": "5089ec5c"
      },
      "source": [
        "## 2) Imports, seeds, metrics & helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a01e2b4",
      "metadata": {
        "id": "1a01e2b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, random, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from fairlearn.metrics import MetricFrame, true_positive_rate, true_negative_rate\n",
        "\n",
        "def equal_opportunity(y_true, y_pred, sensitive_features):\n",
        "    mf = MetricFrame(metrics=true_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features=sensitive_features)\n",
        "    return {\"group\": mf.by_group.to_dict(), \"gap\": float(mf.difference(method='between_groups'))}\n",
        "\n",
        "def equalized_odds(y_true, y_pred, sensitive_features):\n",
        "    tpr = MetricFrame(metrics=true_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features=sensitive_features)\n",
        "    tnr = MetricFrame(metrics=true_negative_rate, y_true=y_true, y_pred=y_pred, sensitive_features=sensitive_features)\n",
        "    return {\n",
        "        \"tpr_gap\": float(tpr.difference(method='between_groups')),\n",
        "        \"tnr_gap\": float(tnr.difference(method='between_groups')),\n",
        "        \"tpr_by_group\": tpr.by_group.to_dict(),\n",
        "        \"tnr_by_group\": tnr.by_group.to_dict(),\n",
        "    }\n",
        "\n",
        "def summarize_metrics(y_true, y_score, y_pred, sensitive):\n",
        "    out = {\"accuracy\": float(accuracy_score(y_true, y_pred))}\n",
        "    try:\n",
        "        out[\"auc\"] = float(roc_auc_score(y_true, y_score))\n",
        "    except Exception:\n",
        "        out[\"auc\"] = None\n",
        "    eo = equal_opportunity(y_true, y_pred, sensitive)\n",
        "    eod = equalized_odds(y_true, y_pred, sensitive)\n",
        "    out[\"eq_opp_gap\"] = eo[\"gap\"]\n",
        "    out[\"eq_odds_tpr_gap\"] = eod[\"tpr_gap\"]\n",
        "    out[\"eq_odds_tnr_gap\"] = eod[\"tnr_gap\"]\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FIXED Employment loader: no df_from_acs; align sens via df_filter ---\n",
        "import os\n",
        "import pandas as pd\n",
        "from folktables import ACSDataSource, ACSEmployment\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "def load_folktables_employment(states=('CA','TX','NY'), year=2018, horizon='1-Year'):\n",
        "    # 1) Load raw ACS person data to a writable cache\n",
        "    ds = ACSDataSource(survey_year=year, horizon=horizon, survey='person', root_dir=DATA_ROOT)\n",
        "    acs = ds.get_data(states=list(states), download=True)\n",
        "\n",
        "    # 2) Use the problem’s df_from_acs method to get the filtered DataFrame\n",
        "    # mask = ACSEmployment.df_filter(acs) # This method does not exist in this version\n",
        "    # keep only rows/cols the problem will use + our sens cols; drop NA to match df_to_numpy internals\n",
        "    # cols_needed = ACSEmployment.features + [ACSEmployment.target, 'SEX', 'RAC1P']\n",
        "    # df_f = acs.loc[mask, cols_needed].dropna(subset=ACSEmployment.features + [ACSEmployment.target])\n",
        "\n",
        "    df_f = ACSEmployment.df_from_acs(acs)\n",
        "\n",
        "    # 3) Convert to numpy via the problem definition\n",
        "    X_np, y_np, _ = ACSEmployment.df_to_numpy(df_f)\n",
        "\n",
        "    # 4) Build outputs\n",
        "    X = pd.DataFrame(X_np, columns=ACSEmployment.features)\n",
        "    y = pd.Series(y_np.astype(int), name='is_employed')\n",
        "\n",
        "    # Extract sensitive features directly from the filtered DataFrame\n",
        "    sens = df_f[['SEX', 'RAC1P']].astype(int).rename(columns={'SEX':'sex','RAC1P':'race'})\n",
        "    sens['sex'] = sens['sex'].map({1: 'Male', 2: 'Female'})\n",
        "\n",
        "    # 5) Hard-guard equal lengths (paranoia, but safe)\n",
        "    n = min(len(X), len(y), len(sens))\n",
        "    return X.iloc[:n].reset_index(drop=True), y.iloc[:n].reset_index(drop=True), sens.iloc[:n].reset_index(drop=True)\n",
        "\n",
        "# Now call:\n",
        "X_emp, y_emp, s_emp = load_folktables_employment()\n",
        "print(\"Employment shapes:\", X_emp.shape, y_emp.shape, s_emp.shape)"
      ],
      "metadata": {
        "id": "7Tckq_LsFXrG"
      },
      "id": "7Tckq_LsFXrG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cbc2732f",
      "metadata": {
        "id": "cbc2732f"
      },
      "source": [
        "## 3) Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === SECTION: Folktables – Employment (robust loader + full export) ===\n",
        "import os, json\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt, matplotlib.patheffects as pe\n",
        "from pathlib import Path\n",
        "\n",
        "# 0) Ensure folktables is available\n",
        "try:\n",
        "    from folktables import ACSDataSource, ACSEmployment\n",
        "except ModuleNotFoundError:\n",
        "    !pip -q install folktables==0.0.12\n",
        "    from folktables import ACSDataSource, ACSEmployment\n",
        "\n",
        "SEED = 42\n",
        "EXPORT_DIR = Path(\"/content/exports\"); EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DATA_ROOT = \"/content/data\"  # force a writable, persistent cache in Colab\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "def load_folktables_employment(states=('CA','TX','NY'), year=2018, horizon='1-Year'):\n",
        "    \"\"\"\n",
        "    Robust Employment loader:\n",
        "    - Forces Folktables cache to /content/data\n",
        "    - Ensures equal lengths across X, y, sens\n",
        "    - Returns clean pandas objects (X, y, sens)\n",
        "    \"\"\"\n",
        "    ds = ACSDataSource(survey_year=year, horizon=horizon, survey='person', root_dir=DATA_ROOT)\n",
        "    # Force download (safe to call repeatedly)\n",
        "    acs = ds.get_data(states=list(states), download=True)\n",
        "\n",
        "    # Problem spec -> numpy\n",
        "    raw_df = ACSEmployment.df_from_acs(acs)\n",
        "    X_np, y_np, _ = ACSEmployment.df_to_numpy(raw_df)\n",
        "\n",
        "    # Features/labels\n",
        "    X = pd.DataFrame(X_np, columns=ACSEmployment.features)\n",
        "    y = pd.Series(y_np.astype(int), name=\"is_employed\")\n",
        "\n",
        "    # Sensitive features (sex/race)\n",
        "    sens = raw_df[[\"SEX\",\"RAC1P\"]].copy()\n",
        "    sens.columns = [\"sex\",\"race\"]\n",
        "    sens[\"sex\"] = sens[\"sex\"].map({1:\"Male\", 2:\"Female\"})\n",
        "\n",
        "    # Enforce equal length (rare, but defensive)\n",
        "    n = min(len(X), len(y), len(sens))\n",
        "    X = X.iloc[:n].reset_index(drop=True)\n",
        "    y = y.iloc[:n].reset_index(drop=True)\n",
        "    sens = sens.iloc[:n].reset_index(drop=True)\n",
        "\n",
        "    return X, y, sens\n",
        "\n",
        "# 1) Load data\n",
        "X_emp, y_emp, s_emp = load_folktables_employment()\n",
        "print(\"Employment shapes:\", X_emp.shape, y_emp.shape, s_emp.shape)\n",
        "display(s_emp.head())\n",
        "\n",
        "# 2) Run fairness variants (uses your existing helpers):\n",
        "#    - run_tabular_baseline\n",
        "#    - run_tabular_with_dsap_reweighting\n",
        "#    - run_equalized_odds_reduction\n",
        "#    - postprocess_equalized_odds_on_scores\n",
        "#    - summarize_metrics\n",
        "metrics_emp = {}\n",
        "\n",
        "m_base, y_b, s_b, p_b, sens_b = run_tabular_baseline(X_emp, y_emp, s_emp[[\"sex\"]])\n",
        "metrics_emp[\"baseline\"] = m_base\n",
        "\n",
        "m_dsap, y_d, s_d, p_d, sens_d = run_tabular_with_dsap_reweighting(X_emp, y_emp, s_emp[[\"sex\"]])\n",
        "metrics_emp[\"pre_DSAP\"] = m_dsap\n",
        "\n",
        "m_eo, y_e, s_e, p_e, sens_e = run_equalized_odds_reduction(X_emp, y_emp, s_emp[[\"sex\"]])\n",
        "metrics_emp[\"inproc_EO\"] = m_eo\n",
        "\n",
        "m_post, y_p, scores_p, y_pr_p, s_p = postprocess_equalized_odds_on_scores(X_emp, y_emp, s_emp[[\"sex\"]])\n",
        "metrics_emp[\"post_EO\"] = m_post\n",
        "\n",
        "# 3) Summarize to DataFrame (Accuracy, AUC, EO Gap) and tag dataset\n",
        "df_emp = pd.DataFrame(metrics_emp).T\n",
        "df_emp[\"Dataset\"] = \"ACS Employment\"\n",
        "df_emp = df_emp[[\"Dataset\",\"accuracy\",\"auc\",\"eq_opp_gap\"]].rename(\n",
        "    columns={\"accuracy\":\"Accuracy\",\"auc\":\"AUC\",\"eq_opp_gap\":\"EO Gap\"}\n",
        ")\n",
        "display(df_emp)\n",
        "\n",
        "# 4) Append/merge into overall summary table and write CSV + LaTeX\n",
        "sum_csv = EXPORT_DIR / \"results_summary_all.csv\"\n",
        "if sum_csv.exists():\n",
        "    df_all = pd.read_csv(sum_csv)\n",
        "    # drop old Employment rows if re-running\n",
        "    df_all = df_all[df_all[\"Dataset\"] != \"ACS Employment\"]\n",
        "else:\n",
        "    df_all = pd.DataFrame(columns=[\"Dataset\",\"Method\",\"Accuracy\",\"AUC\",\"EO Gap\"])\n",
        "\n",
        "df_emp_out = df_emp.reset_index().rename(columns={\"index\":\"Method\"})\n",
        "df_all = pd.concat([df_all, df_emp_out], ignore_index=True)\n",
        "df_all.to_csv(sum_csv, index=False)\n",
        "\n",
        "latex_str = df_all.round({\"Accuracy\":4,\"AUC\":4,\"EO Gap\":4}).to_latex(\n",
        "    index=False, escape=False,\n",
        "    caption=\"Summary of utility and fairness across datasets and methods.\",\n",
        "    label=\"tab:summary_all\"\n",
        ")\n",
        "(EXPORT_DIR / \"results_summary_all.tex\").write_text(latex_str)\n",
        "\n",
        "print(\"✅ Wrote:\")\n",
        "print(\"  -\", sum_csv)\n",
        "print(\"  -\", EXPORT_DIR / \"results_summary_all.tex\")\n",
        "display(df_all.tail(10))\n",
        "\n",
        "# 5) Polished Employment trade-off plot (non-overlapping labels) -> PNG/PDF\n",
        "x = df_emp[\"Accuracy\"].values\n",
        "y = df_emp[\"EO Gap\"].values\n",
        "labels = df_emp.index.tolist()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6.5, 5.2), dpi=300)\n",
        "ax.scatter(x, y, s=90, color=\"#007acc\", alpha=0.9, edgecolors=\"k\")\n",
        "halo = [pe.withStroke(linewidth=3, foreground=\"white\")]\n",
        "offsets = {\"baseline\":(10,8),\"pre_DSAP\":(10,-10),\"inproc_EO\":(0,-14),\"post_EO\":(10,6)}\n",
        "for xi, yi, lab in zip(x, y, labels):\n",
        "    dx, dy = offsets.get(lab, (6,6))\n",
        "    ax.annotate(lab, (xi, yi), textcoords=\"offset points\", xytext=(dx, dy),\n",
        "                ha=\"left\", va=\"center\", fontsize=11, path_effects=halo)\n",
        "\n",
        "pad_x = max(0.001, (x.max()-x.min())*0.35)\n",
        "pad_y = max(0.005, (y.max()-y.min())*0.35)\n",
        "ax.set_xlim(x.min()-pad_x, x.max()+pad_x)\n",
        "ax.set_ylim(y.min()-pad_y, y.max()+pad_y)\n",
        "ax.set_xlabel(\"Accuracy (↑)\", fontsize=12)\n",
        "ax.set_ylabel(\"Equal Opportunity Gap (↓)\", fontsize=12)\n",
        "ax.set_title(\"Utility vs Fairness (ACS Employment)\", fontsize=14)\n",
        "ax.grid(True, linestyle=\"--\", alpha=0.25)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPORT_DIR / \"employment_tradeoff_v2.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.savefig(EXPORT_DIR / \"employment_tradeoff_v2.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Saved Employment plots:\",\n",
        "      EXPORT_DIR / \"employment_tradeoff_v2.png\",\n",
        "      \"|\", EXPORT_DIR / \"employment_tradeoff_v2.pdf\")"
      ],
      "metadata": {
        "id": "iYYyN7139vCl"
      },
      "id": "iYYyN7139vCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7f34a26b",
      "metadata": {
        "id": "7f34a26b"
      },
      "source": [
        "### 3.1 Folktables — ACSIncome (auto-download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c176096",
      "metadata": {
        "id": "2c176096"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from folktables import ACSDataSource, ACSIncome\n",
        "\n",
        "def load_folktables_income(states=('CA','TX','NY'), year=2018, horizon='1-Year'):\n",
        "    ds = ACSDataSource(survey_year=year, horizon=horizon, survey='person')\n",
        "    acs = ds.get_data(states=list(states), download=True)  # raw ACS DataFrame\n",
        "\n",
        "    # In folktables==0.0.12, pass the DataFrame directly\n",
        "    X_np, y_np, _ = ACSIncome.df_to_numpy(acs)\n",
        "    X = pd.DataFrame(X_np, columns=ACSIncome.features)\n",
        "    y = pd.Series(y_np.astype(int), name=\"income_gt_50k\")\n",
        "\n",
        "    # --- build sensitive attributes DIRECTLY from X so lengths align ---\n",
        "    # (SEX: 1=Male, 2=Female; RAC1P is numeric race code)\n",
        "    assert \"SEX\" in X.columns and \"RAC1P\" in X.columns, \\\n",
        "        \"Expected SEX and RAC1P in ACSIncome.features but did not find them.\"\n",
        "\n",
        "    sex_raw = X[\"SEX\"].astype(int)\n",
        "    race_raw = X[\"RAC1P\"].astype(int)\n",
        "\n",
        "    sens = pd.DataFrame(index=X.index)\n",
        "    sens[\"sex_num\"] = sex_raw.map({1: 1, 2: 0}).fillna(0).astype(int)   # Male=1, Female=0\n",
        "    sens[\"sex_str\"] = sex_raw.map({1: \"Male\", 2: \"Female\"}).fillna(\"Unknown\")\n",
        "    sens[\"race\"]    = race_raw\n",
        "\n",
        "    # Final sanity\n",
        "    assert len(X) == len(y) == len(sens), f\"Length mismatch: X={len(X)}, y={len(y)}, sens={len(sens)}\"\n",
        "    return X, y, sens\n",
        "\n",
        "# Load\n",
        "X_ft, y_ft, s_ft = load_folktables_income()\n",
        "(X_ft.shape, y_ft.shape, s_ft[['sex_str','sex_num','race']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68cecec",
      "metadata": {
        "id": "b68cecec"
      },
      "source": [
        "### 3.2 COMPAS (manual upload)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9996e501",
      "metadata": {
        "id": "9996e501"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, pandas as pd\n",
        "def load_compas_csv(path=\"/content/data/compas/compas-scores-two-years.csv\"):\n",
        "    assert os.path.exists(path), f\"Missing file at {path}. Download from ProPublica and upload to Colab.\"\n",
        "    df = pd.read_csv(path)\n",
        "    df = df[(df[\"days_b_screening_arrest\"] <= 30) & (df[\"days_b_screening_arrest\"] >= -30)]\n",
        "    df = df[df[\"is_recid\"] != -1]\n",
        "    df = df[df[\"c_charge_degree\"] != \"O\"]\n",
        "    y = df[\"two_year_recid\"].astype(int)\n",
        "    s = df[[\"race\",\"sex\"]].copy()\n",
        "    feats = [\"age\",\"juv_fel_count\",\"juv_misd_count\",\"juv_other_count\",\"priors_count\"]\n",
        "    X = df[feats].copy()\n",
        "    return X, y, s\n",
        "# X_c, y_c, s_c = load_compas_csv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126cb0c9",
      "metadata": {
        "id": "126cb0c9"
      },
      "source": [
        "## 4) Baseline model (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c58996",
      "metadata": {
        "id": "36c58996"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def run_tabular_baseline(X, y, sens, test_size=0.25):\n",
        "    X_tr, X_te, y_tr, y_te, s_tr, s_te = train_test_split(\n",
        "        X, y, sens, test_size=test_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "    pipe = Pipeline([\n",
        "        (\"scaler\", StandardScaler(with_mean=False) if hasattr(X_tr, \"sparse\") else StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    y_sc = pipe.predict_proba(X_te)[:,1]\n",
        "    y_pr = (y_sc >= 0.5).astype(int)\n",
        "    metrics = summarize_metrics(y_te, y_sc, y_pr, sensitive=s_te.iloc[:,0])\n",
        "    return metrics, y_te, y_sc, y_pr, s_te\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac952a4e",
      "metadata": {
        "id": "ac952a4e"
      },
      "source": [
        "## 5) Debiasing #1 — Pre-processing (DSAP-style reweighting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9ad6c7",
      "metadata": {
        "id": "2b9ad6c7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "def dsap_reweight_sample_weights(sensitive_series):\n",
        "    counts = sensitive_series.value_counts()\n",
        "    inv = 1.0 / counts\n",
        "    weights = sensitive_series.map(inv)\n",
        "    return (weights / weights.mean()).values\n",
        "def run_tabular_with_dsap_reweighting(X, y, sens, test_size=0.25):\n",
        "    X_tr, X_te, y_tr, y_te, s_tr, s_te = train_test_split(\n",
        "        X, y, sens, test_size=test_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "    w_tr = dsap_reweight_sample_weights(s_tr.iloc[:,0])\n",
        "    pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=False) if hasattr(X_tr, \"sparse\") else StandardScaler()),\n",
        "                     (\"clf\", LogisticRegression(max_iter=1000))])\n",
        "    pipe.fit(X_tr, y_tr, clf__sample_weight=w_tr)\n",
        "    y_sc = pipe.predict_proba(X_te)[:,1]\n",
        "    y_pr = (y_sc >= 0.5).astype(int)\n",
        "    metrics = summarize_metrics(y_te, y_sc, y_pr, sensitive=s_te.iloc[:,0])\n",
        "    return metrics, y_te, y_sc, y_pr, s_te\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd8c38a7",
      "metadata": {
        "id": "fd8c38a7"
      },
      "source": [
        "## 6) Debiasing #2 — In-processing (Equalized Odds via reductions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab286a4",
      "metadata": {
        "id": "fab286a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
        "def run_equalized_odds_reduction(X, y, sens, test_size=0.25, eps=0.01):\n",
        "    X_tr, X_te, y_tr, y_te, s_tr, s_te = train_test_split(\n",
        "        X, y, sens, test_size=test_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "    base = LogisticRegression(max_iter=1000)\n",
        "    constraint = EqualizedOdds()\n",
        "    mitigator = ExponentiatedGradient(estimator=base, constraints=constraint, eps=eps)\n",
        "    mitigator.fit(X_tr, y_tr, sensitive_features=s_tr.iloc[:,0])\n",
        "    try:\n",
        "        y_sc = mitigator._pmf_predict(X_te)[:,1]\n",
        "    except Exception:\n",
        "        try:\n",
        "            y_sc = mitigator.predict_proba(X_te)[:,1]\n",
        "        except Exception:\n",
        "            y_sc = mitigator.predict(X_te).astype(float)\n",
        "    y_pr = (y_sc >= 0.5).astype(int)\n",
        "    metrics = summarize_metrics(y_te, y_sc, y_pr, sensitive=s_te.iloc[:,0])\n",
        "    return metrics, y_te, y_sc, y_pr, s_te\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614efc2d",
      "metadata": {
        "id": "614efc2d"
      },
      "source": [
        "## 7) Debiasing #3 — Post-processing (ThresholdOptimizer Equalized Odds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0471eb1",
      "metadata": {
        "id": "c0471eb1"
      },
      "outputs": [],
      "source": [
        "\n",
        "from fairlearn.postprocessing import ThresholdOptimizer\n",
        "def run_threshold_optimizer(X, y, sens, test_size=0.25):\n",
        "    X_tr, X_te, y_tr, y_te, s_tr, s_te = train_test_split(\n",
        "        X, y, sens, test_size=test_size, random_state=SEED, stratify=y\n",
        "    )\n",
        "    pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=False) if hasattr(X_tr, \"sparse\") else StandardScaler()),\n",
        "                     (\"clf\", LogisticRegression(max_iter=1000))])\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    scores_tr = pipe.predict_proba(X_tr)[:,1]\n",
        "    scores_te = pipe.predict_proba(X_te)[:,1]\n",
        "    topt = ThresholdOptimizer(estimator=None, constraints=\"equalized_odds\", prefit=True)\n",
        "    topt.fit(scores_tr.reshape(-1,1), y_tr, sensitive_features=s_tr.iloc[:,0])\n",
        "    y_pr = topt.predict(scores_te.reshape(-1,1), sensitive_features=s_te.iloc[:,0])\n",
        "    metrics = summarize_metrics(y_te, scores_te, y_pr, sensitive=s_te.iloc[:,0])\n",
        "    return metrics, y_te, scores_te, y_pr, s_te\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a07a0db",
      "metadata": {
        "id": "2a07a0db"
      },
      "source": [
        "## 8) Run Folktables end-to-end (aligned X/y/sens + numeric sensitive + safe post-processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf22e28",
      "metadata": {
        "id": "aaf22e28"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Section 8: Folktables end-to-end with aligned sensitive handling and safe post-processing ---\n",
        "import numpy as np, pandas as pd, os, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from fairlearn.postprocessing import ThresholdOptimizer\n",
        "\n",
        "def to_numeric_sensitive(s):\n",
        "    if isinstance(s, pd.DataFrame):\n",
        "        s = s.iloc[:, 0]\n",
        "    if s.dtype.kind in (\"O\", \"U\", \"S\", \"b\"):\n",
        "        s = (\n",
        "            s.astype(str).str.strip().str.lower()\n",
        "            .map({\"male\": 1, \"female\": 0, \"1\": 1, \"0\": 0, \"true\": 1, \"false\": 0})\n",
        "        )\n",
        "    s = pd.to_numeric(s, errors=\"coerce\").fillna(0).astype(int)\n",
        "    s.name = \"sens_num\"\n",
        "    return s\n",
        "\n",
        "assert 's_ft' in globals(), \"Run Section 3.1 first.\"\n",
        "s_ft_num = to_numeric_sensitive(s_ft['sex_num']) if 'sex_num' in s_ft.columns else to_numeric_sensitive(s_ft['sex'])\n",
        "\n",
        "SUBSAMPLE_N = None\n",
        "if SUBSAMPLE_N is not None and len(X_ft) > SUBSAMPLE_N:\n",
        "    idx = np.random.RandomState(42).choice(len(X_ft), size=SUBSAMPLE_N, replace=False)\n",
        "    X_ft = X_ft.iloc[idx].reset_index(drop=True); y_ft = y_ft.iloc[idx].reset_index(drop=True); s_ft_num = s_ft_num.iloc[idx].reset_index(drop=True)\n",
        "\n",
        "m_base, y_b, s_b, p_b, sens_b = run_tabular_baseline(X_ft, y_ft, s_ft_num.to_frame())\n",
        "print(\"Baseline:\", m_base)\n",
        "\n",
        "m_dsap, y_d, s_d, p_d, sens_d = run_tabular_with_dsap_reweighting(X_ft, y_ft, s_ft_num.to_frame())\n",
        "print(\"Pre (DSAP-style):\", m_dsap)\n",
        "\n",
        "m_eo, y_e, s_e, p_e, sens_e = run_equalized_odds_reduction(X_ft, y_ft, s_ft_num.to_frame())\n",
        "print(\"In-proc (Equalized Odds):\", m_eo)\n",
        "\n",
        "def postprocess_equalized_odds_on_scores(X, y, sens_series):\n",
        "    \"\"\"\n",
        "    Train a logistic pipeline to get scores, then run Equalized Odds post-processing\n",
        "    on the SCORES ONLY via a dummy estimator that accepts 1-D scores.\n",
        "    This avoids sending scores back through the StandardScaler/Pipeline.\n",
        "    Returns: metrics, Y_eval, SCORES_eval, y_pred_post, S_eval\n",
        "    \"\"\"\n",
        "    # 1) Train a simple scorer\n",
        "    X_tr, X_te, y_tr, y_te, s_tr, s_te = train_test_split(\n",
        "        X, y, sens_series, test_size=0.25, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline([\n",
        "        (\"scaler\", StandardScaler(with_mean=False) if hasattr(X_tr, \"sparse\") else StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(max_iter=3000, solver=\"lbfgs\"))\n",
        "    ])\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    scores_te = pipe.predict_proba(X_te)[:, 1]  # shape (n,)\n",
        "\n",
        "    # 2) Split scores into calibration/evaluation\n",
        "    SCORES_cal, SCORES_eval, Y_cal, Y_eval, S_cal, S_eval = train_test_split(\n",
        "        scores_te.reshape(-1, 1), y_te, s_te, test_size=0.5, random_state=42, stratify=y_te\n",
        "    )\n",
        "\n",
        "    # 3) Dummy estimator that interprets its input as probabilities\n",
        "    class ScoreWrapper:\n",
        "        def fit(self, X, y=None):  # no-op, but kept for API compatibility\n",
        "            return self\n",
        "        def predict_proba(self, X):\n",
        "            p = np.asarray(X).reshape(-1, 1)\n",
        "            p = np.clip(p, 0.0, 1.0)\n",
        "            return np.hstack([1 - p, p])\n",
        "\n",
        "    score_model = ScoreWrapper()\n",
        "\n",
        "    # 4) ThresholdOptimizer on scores (NOT the original pipeline)\n",
        "    topt = ThresholdOptimizer(\n",
        "        estimator=score_model,\n",
        "        constraints=\"equalized_odds\",\n",
        "        prefit=True,\n",
        "        predict_method=\"predict_proba\"\n",
        "    )\n",
        "    topt.fit(SCORES_cal, Y_cal, sensitive_features=S_cal)\n",
        "\n",
        "    # 5) Predict debiased labels on held-out scores\n",
        "    y_pred_post = topt.predict(SCORES_eval, sensitive_features=S_eval)\n",
        "\n",
        "    # 6) Summarize with your helper: use scores for AUC, debiased labels for fairness\n",
        "    metrics = summarize_metrics(Y_eval, SCORES_eval.ravel(), y_pred_post, sensitive=S_eval)\n",
        "    return metrics, Y_eval, SCORES_eval.ravel(), y_pred_post, S_eval\n",
        "\n",
        "m_post, y_p, scores_p, y_pr_p, s_p = postprocess_equalized_odds_on_scores(X_ft, y_ft, s_ft_num)\n",
        "print(\"Post-proc (ThresholdOptimizer on scores):\", m_post)\n",
        "\n",
        "results = {\"baseline\": m_base, \"pre_DSAP\": m_dsap, \"inproc_EO\": m_eo, \"post_EO\": m_post}\n",
        "df_res = pd.DataFrame(results).T; display(df_res)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter(df_res[\"accuracy\"], df_res[\"eq_opp_gap\"])\n",
        "for name, row in df_res.iterrows():\n",
        "    plt.text(row[\"accuracy\"] + 0.001, row[\"eq_opp_gap\"] + 0.001, name)\n",
        "plt.xlabel(\"Accuracy (↑)\"); plt.ylabel(\"Eq. Opportunity Gap (↓)\"); plt.title(\"Folktables: Utility vs Fairness\"); plt.tight_layout()\n",
        "\n",
        "os.makedirs(\"/content/exports\", exist_ok=True)\n",
        "plt.savefig(\"/content/exports/tradeoff_plot.png\", dpi=200); df_res.to_csv(\"/content/exports/results_summary.csv\")\n",
        "print(\"Saved: /content/exports/tradeoff_plot.png and /content/exports/results_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ad2196",
      "metadata": {
        "id": "90ad2196"
      },
      "source": [
        "## 9) Aggregate results, plot trade-off, export for Overleaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8c18d0",
      "metadata": {
        "id": "4f8c18d0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd, matplotlib.pyplot as plt, os\n",
        "results = {\"baseline\": m_base, \"pre_DSAP\": m_dsap, \"inproc_EO\": m_eo, \"post_EO\": m_post}\n",
        "df_res = pd.DataFrame(results).T\n",
        "display(df_res)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter(df_res[\"accuracy\"], df_res[\"eq_opp_gap\"])\n",
        "for name, row in df_res.iterrows():\n",
        "    plt.text(row[\"accuracy\"]+0.001, row[\"eq_opp_gap\"]+0.001, name)\n",
        "plt.xlabel(\"Accuracy (↑)\"); plt.ylabel(\"Eq. Opportunity Gap (↓)\"); plt.title(\"Utility vs Fairness\")\n",
        "plt.tight_layout()\n",
        "os.makedirs(\"/content/exports\", exist_ok=True)\n",
        "plt.savefig(\"/content/exports/tradeoff_plot.png\", dpi=200)\n",
        "df_res.to_csv(\"/content/exports/results_summary.csv\")\n",
        "print(\"Saved: /content/exports/tradeoff_plot.png and /content/exports/results_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "200570d7",
      "metadata": {
        "id": "200570d7"
      },
      "source": [
        "## 10) (Optional) Run COMPAS with race as sensitive attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff40fab7",
      "metadata": {
        "id": "ff40fab7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# X_c, y_c, s_c = load_compas_csv()\n",
        "# m_base_c, *_ = run_tabular_baseline(X_c, y_c, s_c[[\"race\"]]); print(\"COMPAS Baseline:\", m_base_c)\n",
        "# m_dsap_c, *_ = run_tabular_with_dsap_reweighting(X_c, y_c, s_c[[\"race\"]]); print(\"COMPAS DSAP:\", m_dsap_c)\n",
        "# m_eo_c, *_ = run_equalized_odds_reduction(X_c, y_c, s_c[[\"race\"]]); print(\"COMPAS EO:\", m_eo_c)\n",
        "# m_post_c, *_ = run_threshold_optimizer(X_c, y_c, s_c[[\"race\"]]); print(\"COMPAS Post:\", m_post_c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a615b5",
      "metadata": {
        "id": "d3a615b5"
      },
      "source": [
        "## 11) CivilComments (Text) — BERT baseline with fairness metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42a0f6f",
      "metadata": {
        "id": "f42a0f6f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Upload Kaggle 'train.csv' to /content/data/civilcomments/train.csv before running this section.\n",
        "import os, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_civilcomments(path=\"/content/data/civilcomments/train.csv\", sample=8000, identity_preference=(\"male\",\"female\")):\n",
        "    assert os.path.exists(path), f\"Missing CivilComments CSV at {path} (download from Kaggle, file 'train.csv').\"\n",
        "    df = pd.read_csv(path)\n",
        "    if \"toxic\" in df.columns:\n",
        "        df[\"target_bin\"] = (df[\"toxic\"] >= 0.5).astype(int)\n",
        "    elif \"target\" in df.columns:\n",
        "        df[\"target_bin\"] = (df[\"target\"] >= 0.5).astype(int)\n",
        "    else:\n",
        "        raise ValueError(\"Expected 'toxic' or 'target' column in CivilComments.\")\n",
        "    id_cols = [c for c in df.columns if c.lower() in [\"male\",\"female\",\"black\",\"white\",\"asian\",\"christian\",\"jewish\",\"muslim\",\"lgbtq\"]]\n",
        "    if not id_cols:\n",
        "        id_cols = [c for c in df.columns if \"identity\" in c.lower() or c.endswith(\"_identity\")]\n",
        "    if len(id_cols) == 0:\n",
        "        sens = pd.Series([0]*len(df), name=\"sens\")\n",
        "    else:\n",
        "        pick = None\n",
        "        for cand in identity_preference:\n",
        "            if cand in df.columns:\n",
        "                pick = cand; break\n",
        "        if pick is None:\n",
        "            pick = id_cols[0]\n",
        "        sens = (df[pick].fillna(0.0) >= 0.5).astype(int)\n",
        "        sens.name = f\"sens_{pick}\"\n",
        "    texts = df[\"comment_text\"].fillna(\"\")\n",
        "    y = df[\"target_bin\"].astype(int)\n",
        "    if sample and len(df) > sample:\n",
        "        idx = np.random.RandomState(42).choice(len(df), size=sample, replace=False)\n",
        "        texts = texts.iloc[idx]; y = y.iloc[idx]; sens = sens.iloc[idx]\n",
        "    return texts.reset_index(drop=True), y.reset_index(drop=True), sens.reset_index(drop=True)\n",
        "\n",
        "X_text, y_text, sens_text = load_civilcomments()\n",
        "len(X_text), y_text.value_counts(normalize=True).round(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e2ee042",
      "metadata": {
        "id": "5e2ee042"
      },
      "source": [
        "### 11.1 Train a small BERT classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e548b82",
      "metadata": {
        "id": "8e548b82"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "df_text = pd.DataFrame({\"text\": X_text.values, \"label\": y_text.values, \"sens\": sens_text.values})\n",
        "tr_df, te_df = train_test_split(df_text, test_size=0.2, random_state=42, stratify=df_text[\"label\"])\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "def tokenize_batch(batch):\n",
        "    return tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "from datasets import Dataset as HFDataset\n",
        "tr_ds = HFDataset.from_pandas(tr_df[[\"text\",\"label\"]])\n",
        "te_ds = HFDataset.from_pandas(te_df[[\"text\",\"label\"]])\n",
        "tr_enc = tr_ds.map(tokenize_batch, batched=True).remove_columns([\"text\",\"__index_level_0__\"] if \"__index_level_0__\" in tr_ds.column_names else [\"text\"])\n",
        "te_enc = te_ds.map(tokenize_batch, batched=True).remove_columns([\"text\",\"__index_level_0__\"] if \"__index_level_0__\" in te_ds.column_names else [\"text\"])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(DEVICE)\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/cc_out\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=1)[:,1].numpy()\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    try:\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "    except Exception:\n",
        "        auc = None\n",
        "    return {\"accuracy\": acc, \"auc\": -1 if auc is None else auc}\n",
        "trainer = Trainer(model=model, args=args, train_dataset=tr_enc, eval_dataset=te_enc, compute_metrics=compute_metrics)\n",
        "trainer.train(); eval_out = trainer.evaluate(); eval_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8745ca84",
      "metadata": {
        "id": "8745ca84"
      },
      "source": [
        "### 11.2 Evaluate fairness on CivilComments test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0867b8",
      "metadata": {
        "id": "7c0867b8"
      },
      "outputs": [],
      "source": [
        "# --- CivilComments: Utility vs Fairness (single point, polished export) ---\n",
        "import os, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "from pathlib import Path\n",
        "\n",
        "Path(\"/content/exports\").mkdir(exist_ok=True)\n",
        "\n",
        "# --- Get baseline metrics (from RAM or cache) ---\n",
        "cc_loaded = False\n",
        "if 'cc_acc' in globals() and 'cc_eog' in globals():\n",
        "    acc_pt, eog_pt = float(cc_acc), float(cc_eog); cc_loaded = True\n",
        "elif 'cc_acc_base' in globals() and 'cc_eog_base' in globals():\n",
        "    acc_pt, eog_pt = float(cc_acc_base), float(cc_eog_base); cc_loaded = True\n",
        "else:\n",
        "    cache_path = \"/content/exports/civilcomments_metrics.json\"\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"r\") as f:\n",
        "            m = json.load(f)\n",
        "        if m.get(\"cc_acc_base\") is not None and m.get(\"cc_eog_base\") is not None:\n",
        "            acc_pt, eog_pt = float(m[\"cc_acc_base\"]), float(m[\"cc_eog_base\"])\n",
        "            cc_loaded = True\n",
        "\n",
        "if not cc_loaded:\n",
        "    raise RuntimeError(\"CivilComments metrics not found. Run Section 12 or save metrics first.\")\n",
        "\n",
        "# --- Plot ---\n",
        "fig, ax = plt.subplots(figsize=(6.5, 5.2), dpi=300)\n",
        "ax.scatter([acc_pt], [eog_pt], s=90, color=\"#007acc\", alpha=0.9, edgecolors=\"k\")\n",
        "\n",
        "halo = [pe.withStroke(linewidth=3, foreground=\"white\")]\n",
        "ax.annotate(\"Baseline\", (acc_pt, eog_pt),\n",
        "            textcoords=\"offset points\", xytext=(8, 8),\n",
        "            ha=\"left\", va=\"center\", fontsize=11, path_effects=halo)\n",
        "\n",
        "# tight zoom around the point so it doesn't look sparse in IEEE two-column\n",
        "ax.set_xlim(acc_pt - 0.0015, acc_pt + 0.0015)\n",
        "ax.set_ylim(eog_pt - 0.0100, eog_pt + 0.0100)\n",
        "\n",
        "ax.set_xlabel(\"Accuracy (↑)\", fontsize=12)\n",
        "ax.set_ylabel(\"Equal Opportunity Gap (↓)\", fontsize=12)\n",
        "ax.set_title(\"CivilComments: Utility vs Fairness\", fontsize=14)\n",
        "ax.grid(True, linestyle=\"--\", alpha=0.25)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save both formats\n",
        "plt.savefig(\"/content/exports/civilcomments_tradeoff_v2.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.savefig(\"/content/exports/civilcomments_tradeoff_v2.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Saved:\",\n",
        "      \"/content/exports/civilcomments_tradeoff_v2.png\",\n",
        "      \"& /content/exports/civilcomments_tradeoff_v2.pdf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a23932",
      "metadata": {
        "id": "86a23932"
      },
      "source": [
        "### 11.3 Merge text results with tabular summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83769c8b",
      "metadata": {
        "id": "83769c8b"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    results[\"text_bert_baseline\"] = metrics_text\n",
        "except NameError:\n",
        "    results = {\"text_bert_baseline\": metrics_text}\n",
        "df_all = pd.DataFrame(results).T\n",
        "display(df_all)\n",
        "df_all.to_csv(\"/content/exports/results_summary_all.csv\")\n",
        "print(\"Saved: /content/exports/results_summary_all.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade fairlearn==0.12.0\n",
        "# Or if you want the latest version:\n",
        "# !pip install --upgrade fairlearn\n",
        "\n",
        "import fairlearn\n",
        "print(\"Fairlearn version:\", fairlearn.__version__)"
      ],
      "metadata": {
        "id": "Y6k2YSinY3dD"
      },
      "id": "Y6k2YSinY3dD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3719fed1",
      "metadata": {
        "id": "3719fed1"
      },
      "source": [
        "## 12) Text Debiasing — Post-processing (Equalized Odds via ThresholdOptimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de9eaff9",
      "metadata": {
        "id": "de9eaff9"
      },
      "source": [
        "\n",
        "We apply **Equalized Odds** post-processing to the **BERT** model's output probabilities on CivilComments.  \n",
        "This learns **group-specific thresholds** over a calibration split and applies them to the held-out split, aiming to reduce disparities between demographic groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4078aed6",
      "metadata": {
        "id": "4078aed6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Section 12: Post-processing on text scores (Equalized Odds) ---\n",
        "\n",
        "import numpy as np\n",
        "from fairlearn.postprocessing import ThresholdOptimizer\n",
        "\n",
        "# Assumes you already have these from your text model:\n",
        "# probs: 1D numpy array of P(y=1) for the whole split (or use logits -> sigmoid)\n",
        "# te_df: DataFrame with 'label' and sensitive column (e.g., 'sens' or 'target_group')\n",
        "\n",
        "# 1) Prepare arrays (adjust column names if needed)\n",
        "y_all = te_df[\"label\"].to_numpy().astype(int)\n",
        "s_all = te_df[\"sens\"].to_numpy() if \"sens\" in te_df.columns else te_df[\"target_group\"].to_numpy()\n",
        "\n",
        "# Ensure sensitive is numeric 0/1\n",
        "if s_all.dtype.kind in (\"O\", \"U\", \"S\", \"b\"):\n",
        "    s_map = {\"male\":1,\"female\":0,\"1\":1,\"0\":0,\"true\":1,\"false\":0}\n",
        "    s_all = np.array([s_map.get(str(v).strip().lower(), 0) for v in s_all], dtype=int)\n",
        "else:\n",
        "    s_all = np.nan_to_num(s_all).astype(int)\n",
        "\n",
        "# 2) Split scores for calibration vs evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "SCORES_tr, SCORES_te, Y_tr, Y_te, S_tr, S_te = train_test_split(\n",
        "    probs.reshape(-1, 1), y_all, s_all, test_size=0.5, random_state=42, stratify=y_all\n",
        ")\n",
        "\n",
        "# 3) Dummy estimator that treats input as P(y=1)\n",
        "class ScoreWrapper:\n",
        "    def fit(self, X, y=None):  # no-op\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        p = np.asarray(X).reshape(-1, 1)\n",
        "        p = np.clip(p, 0.0, 1.0)\n",
        "        return np.hstack([1 - p, p])\n",
        "\n",
        "score_model = ScoreWrapper()\n",
        "\n",
        "# 4) ThresholdOptimizer on scores (NOT on the text model)\n",
        "topt_text = ThresholdOptimizer(\n",
        "    estimator=score_model,\n",
        "    constraints=\"equalized_odds\",\n",
        "    prefit=True,\n",
        "    predict_method=\"predict_proba\"\n",
        ")\n",
        "topt_text.fit(SCORES_tr, Y_tr, sensitive_features=S_tr)\n",
        "\n",
        "# 5) Predict debiased labels and summarize\n",
        "y_pred_post = topt_text.predict(SCORES_te, sensitive_features=S_te)\n",
        "\n",
        "# If you have a helper like summarize_metrics(y_true, scores, y_pred, sensitive):\n",
        "metrics_text_post = summarize_metrics(Y_te, SCORES_te.ravel(), y_pred_post, sensitive=S_te)\n",
        "print(\"Text post-processing (EO):\", metrics_text_post)\n",
        "\n",
        "# Baseline reference\n",
        "metrics_text_base = summarize_metrics(\n",
        "    labels_all, scores_all, (scores_all >= 0.5).astype(int), sensitive=pd.Series(sens_all)\n",
        ")\n",
        "compare_df = pd.DataFrame(\n",
        "    [metrics_text_base, metrics_text_post],\n",
        "    index=[\"text_bert_baseline\", \"text_postproc_EO\"]\n",
        ")\n",
        "display(compare_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ab293c",
      "metadata": {
        "id": "04ab293c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 12.2 — Export plot and merge results\n",
        "os.makedirs(\"/content/exports\", exist_ok=True)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.scatter([compare_df.loc['text_bert_baseline','accuracy']],\n",
        "            [compare_df.loc['text_bert_baseline','eq_opp_gap']],\n",
        "            label=\"Baseline\")\n",
        "plt.scatter([compare_df.loc['text_postproc_EO','accuracy']],\n",
        "            [compare_df.loc['text_postproc_EO','eq_opp_gap']],\n",
        "            label=\"Post (EO)\")\n",
        "plt.xlabel(\"Accuracy (↑)\")\n",
        "plt.ylabel(\"Eq. Opportunity Gap (↓)\")\n",
        "plt.title(\"CivilComments: Baseline vs Post-Processing\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/exports/civilcomments_text_debiasing.png\", dpi=200)\n",
        "print(\"Saved: /content/exports/civilcomments_text_debiasing.png\")\n",
        "\n",
        "# Update aggregated results table\n",
        "try:\n",
        "    results[\"text_postproc_EO\"] = metrics_text_post\n",
        "    pd.DataFrame(results).T.to_csv(\"/content/exports/results_summary_all.csv\")\n",
        "    print(\"Updated: /content/exports/results_summary_all.csv\")\n",
        "except Exception:\n",
        "    print(\"Note: 'results' dict not found; figure exported only.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 📦 SECTION: CACHE & EXPORT (Final Outputs for Overleaf + PPT)\n",
        "# ============================================================\n",
        "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "EXPORT_DIR = Path(\"/content/exports\")\n",
        "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1️⃣ --- CACHE CivilComments metrics ---\n",
        "metrics_cc = {}\n",
        "if 'cc_acc_base' in globals() and 'cc_eog_base' in globals():\n",
        "    metrics_cc[\"cc_acc_base\"] = float(cc_acc_base)\n",
        "    metrics_cc[\"cc_eog_base\"] = float(cc_eog_base)\n",
        "    metrics_cc[\"cc_auc_base\"] = float(cc_auc_base) if 'cc_auc_base' in globals() else None\n",
        "if 'cc_acc_post' in globals() and 'cc_eog_post' in globals():\n",
        "    metrics_cc[\"cc_acc_post\"] = float(cc_acc_post)\n",
        "    metrics_cc[\"cc_eog_post\"] = float(cc_eog_post)\n",
        "    metrics_cc[\"cc_auc_post\"] = float(cc_auc_post) if 'cc_auc_post' in globals() else None\n",
        "\n",
        "cache_file = EXPORT_DIR / \"civilcomments_metrics.json\"\n",
        "with open(cache_file, \"w\") as f:\n",
        "    json.dump(metrics_cc, f, indent=2)\n",
        "print(f\"✅ Cached CivilComments metrics → {cache_file}\")\n",
        "\n",
        "# 2️⃣ --- LOAD CivilComments metrics if needed ---\n",
        "if not metrics_cc and cache_file.exists():\n",
        "    with open(cache_file, \"r\") as f:\n",
        "        metrics_cc = json.load(f)\n",
        "    print(\"✅ Loaded metrics from cache:\", metrics_cc)\n",
        "\n",
        "# 3️⃣ --- Folktables trade-off plot ---\n",
        "assert 'df_res' in globals(), \"df_res missing — please rerun Section 8\"\n",
        "fig, ax = plt.subplots(figsize=(6.5, 5.2), dpi=300)\n",
        "x = df_res[\"accuracy\"].values; y = df_res[\"eq_opp_gap\"].values\n",
        "ax.scatter(x, y, s=70)\n",
        "for xi, yi, lab in zip(x, y, df_res.index):\n",
        "    ax.annotate(lab, (xi, yi), textcoords=\"offset points\", xytext=(6, 6), fontsize=10)\n",
        "padx, pady = max(0.0008, (x.max()-x.min())*0.3), max(0.004, (y.max()-y.min())*0.3)\n",
        "ax.set_xlim(x.min()-padx, x.max()+padx); ax.set_ylim(y.min()-pady, y.max()+pady)\n",
        "ax.set_xlabel(\"Accuracy (↑)\"); ax.set_ylabel(\"Eq. Opportunity Gap (↓)\")\n",
        "ax.set_title(\"Utility vs Fairness (ACS Income)\")\n",
        "ax.grid(True, alpha=0.25); plt.tight_layout()\n",
        "plt.savefig(EXPORT_DIR / \"tradeoff_plot.png\", dpi=300)\n",
        "plt.close(fig)\n",
        "\n",
        "# 4️⃣ --- CivilComments plots ---\n",
        "if metrics_cc:\n",
        "    base_acc, base_eog = metrics_cc.get(\"cc_acc_base\"), metrics_cc.get(\"cc_eog_base\")\n",
        "    post_acc, post_eog = metrics_cc.get(\"cc_acc_post\"), metrics_cc.get(\"cc_eog_post\")\n",
        "\n",
        "    # Single baseline plot\n",
        "    fig, ax = plt.subplots(figsize=(6.5, 5.2), dpi=300)\n",
        "    ax.scatter(base_acc, base_eog, s=80)\n",
        "    ax.annotate(\"Baseline\", (base_acc, base_eog), textcoords=\"offset points\", xytext=(6, 6), fontsize=10)\n",
        "    ax.set_xlim(base_acc-0.0015, base_acc+0.0015); ax.set_ylim(base_eog-0.01, base_eog+0.01)\n",
        "    ax.set_xlabel(\"Accuracy (↑)\"); ax.set_ylabel(\"Eq. Opportunity Gap (↓)\")\n",
        "    ax.set_title(\"CivilComments: Utility vs Fairness\")\n",
        "    ax.grid(True, alpha=0.25); plt.tight_layout()\n",
        "    plt.savefig(EXPORT_DIR / \"civilcomments_tradeoff.png\", dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Baseline vs Post-processing plot\n",
        "    if post_acc is not None and post_eog is not None:\n",
        "        xs, ys = np.array([base_acc, post_acc]), np.array([base_eog, post_eog])\n",
        "        fig, ax = plt.subplots(figsize=(6.5, 5.2), dpi=300)\n",
        "        ax.scatter(xs[0], ys[0], s=80, label=\"Baseline\")\n",
        "        ax.scatter(xs[1], ys[1], s=80, label=\"Post (EO)\")\n",
        "        for xi, yi, lab in zip(xs, ys, [\"Baseline\", \"Post (EO)\"]):\n",
        "            ax.annotate(lab, (xi, yi), textcoords=\"offset points\", xytext=(6, 6), fontsize=10)\n",
        "        ax.set_xlim(xs.min()-0.0008, xs.max()+0.0008)\n",
        "        ax.set_ylim(ys.min()-0.01, ys.max()+0.01)\n",
        "        ax.set_xlabel(\"Accuracy (↑)\"); ax.set_ylabel(\"Eq. Opportunity Gap (↓)\")\n",
        "        ax.set_title(\"CivilComments: Baseline vs Post-Processing\")\n",
        "        ax.grid(True, alpha=0.25); ax.legend(); plt.tight_layout()\n",
        "        plt.savefig(EXPORT_DIR / \"civilcomments_text_debiasing.png\", dpi=300)\n",
        "        plt.close(fig)\n",
        "else:\n",
        "    print(\"⚠️ CivilComments metrics not available; skipping text plots.\")\n",
        "\n",
        "# 5️⃣ --- Summary Table (LaTeX + CSV) ---\n",
        "rows = []\n",
        "for name, r in df_res.iterrows():\n",
        "    rows.append({\n",
        "        \"Dataset\": \"ACS Income\",\n",
        "        \"Method\": name,\n",
        "        \"Accuracy\": r.get(\"accuracy\", np.nan),\n",
        "        \"AUC\": r.get(\"auc\", np.nan),\n",
        "        \"EO Gap\": r.get(\"eq_opp_gap\", np.nan)\n",
        "    })\n",
        "if metrics_cc:\n",
        "    rows.append({\"Dataset\":\"CivilComments\",\"Method\":\"Baseline\",\"Accuracy\":base_acc,\"AUC\":metrics_cc.get(\"cc_auc_base\"),\"EO Gap\":base_eog})\n",
        "    if post_acc is not None:\n",
        "        rows.append({\"Dataset\":\"CivilComments\",\"Method\":\"Post (EO)\",\"Accuracy\":post_acc,\"AUC\":metrics_cc.get(\"cc_auc_post\"),\"EO Gap\":post_eog})\n",
        "\n",
        "tbl = pd.DataFrame(rows)\n",
        "for col in [\"Accuracy\",\"AUC\",\"EO Gap\"]:\n",
        "    tbl[col] = tbl[col].astype(float).round(4)\n",
        "\n",
        "latex_str = tbl.to_latex(index=False, float_format=\"%.4f\",\n",
        "                         caption=\"Summary of utility and fairness across datasets and methods.\",\n",
        "                         label=\"tab:summary\")\n",
        "\n",
        "(tbl_path := EXPORT_DIR / \"results_summary_all.tex\").write_text(latex_str)\n",
        "tbl.to_csv(EXPORT_DIR / \"results_summary_all.csv\", index=False)\n",
        "print(\"✅ Saved plots & tables to\", EXPORT_DIR)\n",
        "display(tbl)"
      ],
      "metadata": {
        "id": "34XmGqFpbhN1"
      },
      "id": "34XmGqFpbhN1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}